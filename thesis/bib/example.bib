
  

@article{Kate,
    author = {Kate, Rohit},
    year = {2015},
    month = {05},
    pages = {},
    title = {Using dynamic time warping distances as features for improved time series classification},
    volume = {30},
    journal = {Data Mining and Knowledge Discovery},
    doi = {10.1007/s10618-015-0418-x}
}

@article{FbProphet,
    author = {Taylor, Sean and Letham, Benjamin},
    year = {2017},
    month = {09},
    pages = {},
    title = {Forecasting at Scale},
    volume = {72},
    journal = {The American Statistician},
    doi = {10.1080/00031305.2017.1380080}
}

@article{IWANA2017268,
    title = "Efficient temporal pattern recognition by means of dissimilarity space embedding with discriminative prototypes",
    journal = "Pattern Recognition",
    volume = "64",
    pages = "268 - 276",
    year = "2017",
    issn = "0031-3203",
    doi = "https://doi.org/10.1016/j.patcog.2016.11.013",
    url = "http://www.sciencedirect.com/science/article/pii/S0031320316303739",
    author = "Brian Kenji Iwana and Volkmar Frinken and Kaspar Riesen and Seiichi Uchida",
    keywords = "Temporal patterns, Online digit classification, Dissimilarity representation, Ensemble classification, Dissimilarity space embedding",
    abstract = "Dissimilarity space embedding (DSE) presents a method of representing data as vectors of dissimilarities. This representation is interesting for its ability to use a dissimilarity measure to embed various patterns (e.g. graph patterns with different topology and temporal patterns with different lengths) into a vector space. The method proposed in this paper uses a dynamic time warping (DTW) based DSE for the purpose of the classification of massive sets of temporal patterns. However, using large data sets introduces the problem of requiring a high computational cost. To address this, we consider a prototype selection approach. A vector space created by DSE offers us the ability to treat its independent dimensions as features allowing for the use of feature selection. The proposed method exploits this and reduces the number of prototypes required for accurate classification. To validate the proposed method we use two-class classification on a data set of handwritten on-line numerical digits. We show that by using DSE with ensemble classification, high accuracy classification is possible with very few prototypes."
}

@Article{Pevn√Ω2016,
    author={Pevn{\'y}, Tom{\'a}{\v{s}}},
    title={Loda: Lightweight on-line detector of anomalies},
    journal={Machine Learning},
    year={2016},
    month={Feb},
    day={01},
    volume={102},
    number={2},
    pages={275-304},
    abstract={In supervised learning it has been shown that a collection of weak classifiers can result in a strong classifier with error rates similar to those of more sophisticated methods. In unsupervised learning, namely in anomaly detection such a paradigm has not yet been demonstrated despite the fact that many methods have been devised as counterparts to supervised binary classifiers. This work partially fills the gap by showing that an ensemble of very weak detectors can lead to a strong anomaly detector with a performance equal to or better than state of the art methods. The simplicity of the proposed ensemble system (to be called Loda) is particularly useful in domains where a large number of samples need to be processed in real-time or in domains where the data stream is subject to concept drift and the detector needs to be updated on-line. Besides being fast and accurate, Loda is also able to operate and update itself on data with missing variables. Loda is thus practical in domains with sensor outages. Moreover, Loda can identify features in which the scrutinized sample deviates from the majority. This capability is useful when the goal is to find out what has caused the anomaly. It should be noted that none of these favorable properties increase Loda's low time and space complexity. We compare Loda to several state of the art anomaly detectors in two settings: batch training and on-line training on data streams. The results on 36 datasets from UCI repository illustrate the strengths of the proposed system, but also provide more insight into the more general questions regarding batch-vs-on-line anomaly detection.},
    issn={1573-0565},
    doi={10.1007/s10994-015-5521-0},
    url={https://doi.org/10.1007/s10994-015-5521-0}
}

@article{JMLR:v21:20-091,
  author  = {Romain Tavenard and Johann Faouzi and Gilles Vandewiele and
             Felix Divo and Guillaume Androz and Chester Holtz and
             Marie Payne and Roman Yurchak and Marc Ru{\ss}wurm and
             Kushal Kolar and Eli Woods},
  title   = {Tslearn, A Machine Learning Toolkit for Time Series Data},
  journal = {Journal of Machine Learning Research},
  year    = {2020},
  volume  = {21},
  number  = {118},
  pages   = {1-6},
  url     = {http://jmlr.org/papers/v21/20-091.html}
}

@article{scikit-learn,
 title={Scikit-learn: Machine Learning in {P}ython},
 author={Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V.
         and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P.
         and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and
         Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
 journal={Journal of Machine Learning Research},
 volume={12},
 pages={2825--2830},
 year={2011}
}

@inproceedings{sklearn_api,
  author    = {Lars Buitinck and Gilles Louppe and Mathieu Blondel and
               Fabian Pedregosa and Andreas Mueller and Olivier Grisel and
               Vlad Niculae and Peter Prettenhofer and Alexandre Gramfort
               and Jaques Grobler and Robert Layton and Jake VanderPlas and
               Arnaud Joly and Brian Holt and Ga{\"{e}}l Varoquaux},
  title     = {{API} design for machine learning software: experiences from the scikit-learn
               project},
  booktitle = {ECML PKDD Workshop: Languages for Data Mining and Machine Learning},
  year      = {2013},
  pages = {108--122},
}