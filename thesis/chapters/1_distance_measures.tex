\chapter{Time Series Data Mining For Visual Analysis}
Even though most of the data around us have a temporal nature, we often transform them into spatial data by either fixing time or ignoring time completely. It is not because the time aspect is not essential, though it makes all tasks significantly more challenging as it increases the number of data points and adding relations between them.  For example, it is straightforward to compare temperatures in two places at a fixed time. However, once we want to compare their temperature profiles throughout the year, we cannot omit the temporal aspect of the data. Because the temporal aspect adds relations between points and increases the dimensionality of the data, the analysis becomes significantly more difficult. This effect is multiplied once we work with a dataset of time series \cite{met:vis-data-help}. Mostly there, we see an opportunity where visual exploration could help.

This chapter will discuss the right choices for measuring the distance between time series and examine how to effectively apply them in the subsequent visual analysis. Then it goes thru the clustering and anomaly detection methods available for large datasets. However, before we dive into the chapter, we will clarify the basic terminology.

Having a set of objects $X$ we define the \textit{distance function} as:
\begin{equation}
    d: X \times X \rightarrow R^{+}_0 
\end{equation}
The resulting non-negative real value is called a \textit{distance} or \textit{dissimilarity}. We call a distance function a metric if and only if it satisfies the following three axioms:
\begin{enumerate}
    \item $\forall x, y \in X: d(x, y) = 0 \Leftrightarrow x = y $ (identity of indiscernibles)
    \item $\forall x, y \in X: d(x, y) = d(y, x)$ (symmetry)
    \item $\forall x, y, z \in X: d(x, y) + d(y, z) \ge d(x, z)$ (triangle inequality)
\end{enumerate}
A \textit{semi-metric} is a distance function that satisfies the first two axioms, but it does not guarantee the triangle inequality.
\textit{A time series} is then defined as a sequence of data points ($X_t$) indexed by time ($t$).

\section{Distance Measures for Time Series}
In a typical scenario, a distance metric computes the distance for one specific data type (categorical, spatial, numerical). However, time series are a combination of multiple data types as each point has an order in time, and we have to take it into account when computing the distance.

To the best of our knowledge, there are over 30 used distance measures for time series today. As there is no "universally best" metric \cite{met:universal}, we will go through the different types of distance measures used for time series, following the categorization proposed by Esling and Agon \cite{met:classification}.

from simple and universal to more advanced and domain-specific. We will For a distance function categorization, we are following the one proposed by Esling and Agon \cite{met:classification}.

\subsection{Shape-Based Distance Measures}
Shape-based distance measures compare the overall shape of the time series \cite{met:classification}. They are the closest derivation of commonly used distance measures adjusted for the time series data, making them easy to implement and understand. Regardless of their simple nature, they generally yield very competitive results and are considered a gold standard of time series distance measures \cite{met:comparison-new, met:dtw-best-0, met:dtw-best-1, met:universal, met:fDTW}.

\subsubsection{Euclidian Distance and Other $L_p$ Norms}
The most basic and widely used distance measures for spatial and temporal data are Euclidian distance (ED) and other $L_p$ norms \cite{met:lp-norm, met:lp-norm-usage}.

A distance measure on a set of objects $X$ is a function $d:X\times~X\rightarrow[0; \infty)$.
Given the two time series $X$ and $Y$, where $X=(x_1; x_2; ...; x_n)$ and $Y=(y_1; y_2;...; y_m)$, we define $L_p$ norms as listed in Table~\ref{tab:lp-norms}.
\begin{table}[!htbp]
\begin{tabular}{ll}
\textbf{Norm}          & \textbf{Definition}                               \\ \hline
$L_1$ - Manhattan      & $\sum^{n}_{i=1}|x_i - y_i|$                       \\
$L_2$ - Euclidian      & $\sqrt{\sum^{n}_{i=1}(x_i - y_i)^2}$              \\
$L_p$ - Minkowski      & $\sqrt[p]{\sum^{n}_{i=1}(x_i - y_i)^\frac{1}{p}}$ \\
$L_\infty$ - Infinite  & $\max_{i=1..n}|x_i - y_i|$
\end{tabular}
\caption{$L_p$ norms for time series.b}
\label{tab:lp-norms}
\end{table}

Although the $L_p$ norms perform well, especially on larger datasets \cite{met:universal, met:comparison-new, met:met-comparison}, they require time series of equal length and tend to be fragile to the noise, shifts, and different speeds in time series \cite{met:classification}.

\subsubsection{Dynamic Time Warping}
An elegant solution to the above-mentioned limitations of the $L_p$ norms are elastic measures, especially the Dynamic Time Warping (DTW) \cite{met:dtw}. DTW is a method to compute a distance between a pair of time series. When computing DTW distance between time series $X$ and $Y$, it finds the best alignment between them as a minimal cost warping path ($W=w_1 ,...,w_k ,...,w_K$) in $m \times n$ matrix. Elements of the matrix represent the cost to align two corresponding points ($(x_i - y_j)^2$). The warping path starts in the bottom-left corner and ends in the upper-right one (Fig. \ref{fig:dtw_warping}). It is formally defined as:
\begin{equation}
    DTW(X; Y) = \argmin_{W=w_1 ,...,w_k ,...,w_K} \sqrt{ \sum^{K}_{k=1; w_k=(i; j)} (x_i - y_j)^2}
\end{equation}

When comparing DTW with the Euclidian distance, DTW finds the best possible alignment between time series (Fig. \ref{fig:Euclid_vs_DTW}). That allows us to reasonably compare time series that could have different lengths, speeds, and shift \cite{met:dtw, met:dtw-window}.

Dynamic time warping is considered one of the best distance measures for time series and regularly outperforms other more sophisticated approaches \cite{met:universal, met:comparison-new, met:dtw-best-1, met:dtw-best-0}, but its use comes with two main concerns:
\begin{enumerate}
    \item In practice, DTW is solved by dynamic programming with $O(n^2)$ time complexity.
    \item DTW is a semi-metric -- it does not guarantee triangle inequality, limiting the number of optimization techniques and structures we can use.  
\end{enumerate}

\begin{figure}[ht]
     \centering
     \begin{subfigure}[b]{0.495\textwidth}
        \centering
        \includesvg[width=\textwidth]{img/dtw_warping_path.svg}
        \caption{}
        \label{fig:dtw_warping}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.495\textwidth}
        \centering
        \includesvg[width=\textwidth]{img/dtw_vs_euclid.svg}
        \caption{}
        \label{fig:Euclid_vs_DTW}
     \end{subfigure}
    \caption{
        Fig.~\ref{fig:dtw_warping} displays the optimal warping path between two time series. The darkness of color encodes the distance between the corresponding data points (darker is further away). Fig.~\ref{fig:Euclid_vs_DTW} shows the alignment of data points between two time series using the Euclidian distance and DTW.
    }
\end{figure}

The time complexity of this algorithm becomes an issue when operating with long time series and their large amounts. As a solution to the DTW algorithm's time complexity, it is possible to either use lower and upper bounding constraints on the maximal allowed warping path \cite{met:dtw-koegh-rata} or use a temporal constraint on the warping window size \cite{met:dtw-window}. Among the most used are Sakoe-Chiba \cite{met:dtw-window}, Itakura \cite{met:dtw-itakura}, and upper bound to discard complex paths \cite{met:DTW-zheng}. Another option is to use an approximate algorithm proposed by Salvador and Chan \cite{met:FastDTW} called FastDTW, where the warping path is recursively projected and optimized from a lower resolution. This algorithm has linear time complexity. Using the bounding or approximate algorithm comes with faster computational speed and could help with better accuracy due to generalization \cite{met:fDTW}.

DTW is a semi-metric that makes it rather problematic to use with many machine learning algorithms that require a metric space. As there does not exist any complete solution to this issue, there are only partial answers, like Feature DTW from \textcite{met:fDTW}. As we want to use a wide variety of algorithms for visual exploration, we will discuss this in more detail in the following section.

\subsubsection{SpADe, DISSIM, and OSB}
In 2007, three new distance measures for time series were proposed. The first one is the Spatial Assembling Distance (SpADe), a distance measure used for streaming data that recognizes matching patterns concerning shifting and scaling in temporal and amplitude axes \cite{met:spade}.

For measuring similarity between trajectories with different sampling rates, Frentzos et al. \cite{met:dissim} proposed the DISSIM as an approximation of the integral of Euclidian distance. 

Because data tend to be noisy and include many outliers, Latecki et al. \cite{met:osb} proposed Optimal Subsequence Bijection (OSB) for similarity search. This method automatically finds the most suitable subsequence for a sensible comparison. Its disadvantage is a higher computational cost.

\subsection{Other Distance Measures}
Except for the shape-based metrics used for time series data, other methods could be used for the distance calculation. These types of measures focus on specific properties shared among time series.
 
\textit{Edit-based} distances were originally designed for string comparison. The idea is to find the minimal number of basic string operations (insertion, deletion, and substitution) by which we can transform one string to another.  The most popular are techniques measuring the similarity by comparing the longest common subsequences (LCSS) \cite{met:LCSS0, met:LCSS1, met:swale}. They are robust to outliers and noise in the data. Similar options are the Edit Distance on Real Sequence (EDR) \cite{met:edr} and Edit Distance with a Real Penalty (ERP) \cite{met:erp}. EDR finds the minimal number of edit operations to convert time series and penalizes unmatched regions' gaps by their lengths. On the other hand, ERP is a combination of DTW and EDR approaches. It uses the Manhattan distance as a penalization for local shifting. Edit-based distances tend to perform well, but generally are outperformed by DTW-like measures \cite{met:dtw-best-0, met:dtw-best-1}. 

\textit{Feature-based} distance measures use features extracted from the original time series, such as correlation \cite{met:coss-col} or coefficients from discrete Fourier transformation and discrete wavelet transformation \cite{met:dft-dwt}. It is possible to use them in specific applications but overly show worse results than other techniques \cite{met:comparison-new}.

\textit{Model-based} measures are an option for very long sequences, but they require prior knowledge of the time series generating process \cite{met:classification}. After choosing and fitting the parametric temporal model on time series, the distance measure is a likelihood that time series came from the same model. Most standard models are using Hidden Markov models or ARMA models \cite{met:hmm}.

\textit{Compression-based} strategies have been succesfully applied in bioinformatics and medical data applications \cite{met:cdm, met:cdm-santos, met:cdm-espoti}. This approach uses the fact that concatenation and compression of similar time series should produce a higher compression ratio than for very diverse ones.


Many of these techniques can outperform more common shape-based measures, particularly in specific domains, but lack wide usage and universality of shape-based approaches. Even though we would primarily use the shape-based measure in this thesis, the proposed techniques for time series analysis can use any underlying distance measure.


\section{Artificial Feature Space Representation}
From the previous section, we can presume that shaped-based distance measures, especially Dynamic Time Warping and its derivates, are robust and accurate. The original DTW algorithm's time complexity can be reduced significantly by using bounding or approximation. However, we still need to consider its semi-metric nature because the triangle inequality is required for many machine learning algorithms \cite{cluster:decade-review}. It significantly reduces the number of available algorithms that we could use, primarily as datasets' size increases. Partial answer to this problem is building a feature space representation, which we could consider to be in a metric space.

\subsection{Feature DTW Transformation}
\textcite{met:fDTW} introduced an idea to build a feature space representation by using DTW. He transforms the original dataset of time series into a new feature space on which it is possible to apply any technique that is used in a metric space.

The transformation works as follows. Having dataset $X$ consisting of time series $(X_1; X_2; ...; X_n)$, we define the Feature DTW transformation as function
\begin{equation}
    f_{fDTW}: X \times X \rightarrow M
\end{equation}
where $M=(m_{ij}) \in \mathbb{R}_{\ge 0}^{n \times n}$ is a distance matrix with $m_{ij}=DTW(X_i; X_j)$.
 
 In his paper, Kate sees an improved accuracy in classification tasks to widely used techniques using DTW directly. Further, he shows that using DTW with bonding and approximation increases the accuracy probably due to better generalization, and points out that any distance measure could be used instead of DTW. Another notable advantage of using the Feature DTW transformation is that it is possible to combine DTW obtained features with other measures and features. In his work, \textcite{met:fDTW} combined DTW features with Euclidian ones, which led to an additional approvement in terms of accuracy. This technique's main drawback is space complexity as the distance matrix $M$ size is $n^2$ regarding the number of time series. 


\subsection{Prototyped Feature DTW Transformation}
The result of Feature DTW transformation on two similar time series should produce very similar and correlated feature vectors. A statistical point suggests that we are adding very little new information while increasing the number of dimensions in the new feature space. An increase in dimensionality leads to a rise in computational and space requirements, while having multiple significantly correlated features is a problem for many machine learning algorithms. Iwana et al. \cite{met:protofDTW} showed that using only a fraction of the original distance matrix $M$ could obtain comparable or better results than using the whole matrix.

Having the dataset $X=(X_1; X_2; ...; X_n)$ and a set of time series $P=(P_1; P_2; ...; P_m)$ such $P \subseteq X$ called prototypes, we define the transformation as function
\begin{equation}
    f_{Iwana-fDTW}: X \times X \rightarrow K
\end{equation}
where $K=(k_{ij}) \in \mathbb{R}_{\ge 0}^{n \times m}$ is a distance matrix with $k_{ij}=DTW(X_i; P_j)$.

As this method strongly depends on prototypes, their selection is crucial for the method's success. They proposed a supervised technique using the \textit{AdaBoost} algorithm and weak learners on one feature at a time. Using this technique, they were able to use a smaller number of components while increasing the classification accuracy. Because this method is available only for supervised problems, they briefly discussed multiple statistical strategies to chose possible prototypes.

All of the approaches in their work require prior computation of the whole distance matrix, which is problematic for larger datasets. In this thesis, we will propose methods that do not require a full distance matrix in advance but rather compute it interactively.

\section{Clustering}
Another part of our work is to determine the clusters in a dataset. Because we are using the Feature DTW transformation, we are not limited to clustering specific only for time series, but we can use a wide variety of clustering algorithms used for spatial data. Because we will be working with large dataset in our analysis, we will list only algorithms that are scalable to a large number of samples and features.

We will separate the clustering algorithms into three categories:
\begin{enumerate}
    \item Distance-based
    \item Methods using the nearest-neighbor graph
    \item Density-based
\end{enumerate}

\subsection{Distance-based Methods}
Distance-based methods are directly using the distance between the data points to determine their affiliations to the clusters. The two main representatives of this category, that scale well  with dataset's size, are \textit{K-means} and \textit{Agglomerative Hierarchical Clustering} methods.

\subsubsection{K-means}
The \textit{k-means} \cite{vis:kmeans} is a simple clustering algorithm that separates the data into $n$ clusters, minimizing the within-cluster sum of squares. It produces convex and isotropic clusters, and it is easily scalable to large datasets. The disadvantages of this method are that we have to define the number of clusters in advance, and in some cases, the clusters in data are neither convex nor isotropic. Never the less it is a solid starting point for any cluster analysis. 

\subsubsection{Agglomerative Hierarchical Clustering}
The \textit{Agglomerative Hierarchical Clustering} \cite{vis:agg-cluster, vis:kmeans} uses a bottom-up strategy to join the two most similar data points or clusters into a single bigger one. The primary representation is a tree-like structure where leaves are single data points, nodes represent clusters, and the root is a single unified cluster. The algorithm selects two closest nodes based on the merge strategy in each iteration and joins them into a single one until the chosen number of clusters is met.

There are four main merge strategies called \textit{linkages}:
\begin{enumerate}
    \item \textit{Single linkage} -- the minimal distance among all tuples of data points from two clusters.
    \item \textit{Complete linkage} -- the maximal distance among all tuples of data points from two clusters.
    \item \textit{Average linkage} -- the average distance between all pairs of data points from two clusters.
    \item \textit{Ward linkage} -- minimization of the within-cluster sum of squares similar to the K-means.
\end{enumerate}
This clustering method is scalable with the increasing number of samples and, based on the linkage type, provides different cluster types. The ward linkage produces the most even-sized clusters among the merging strategies but is usable only with Euclidian metrics while also having a much higher time complexity. If we want to use non-Euclidian metrics, the average linkage is a feasible alternative. Both single linkage and complete linkage are very efficient and scale well on large datasets. The drawback is that they are fragile to noise in the data as they are using only the distance between two points from clusters.

\subsection{Methods Using the Nearest Neighbor Graph}
Another family of clustering algorithms is using the pre-constructed affiliation graph from data points. Usually, it uses the nearest neighbor graph. The two main representatives are the Affinity Propagation and Spectral clustering. 
\subsubsection{Affinity Propagation}
The \textit{Affinity Propagation} \cite{vis:affini} sends messages between data points in the affiliation graph to determine cluster centers and the number of clusters.  The disadvantage is that it is usable only on smaller datasets because of quadratic time complexity, making it unusable on our dataset.

\subsubsection{Spectral Clustering}
The \textit{Spectral Clustering} \cite{vis:spectral} starts with the nearest neighbor graph represented as a similarity matrix where every row represents a data point and its graph distance to the other points. After selecting the number of clusters $n$, it computes the optimal graph cuts to create a connected component for every cluster. The drawback of spectral clustering is that it can be efficiently computed only for a small number of clusters.

\subsection{Density-based Methods}
The last group of clustering algorithms, density-based methods, uses the difference between dense and sparse regions to determine the optimal clustering. 
The main challenge of these methods is to determine the correct density estimate of the dataset, which is problematic for high-dimensional datasets. As the number of dimensions increases, the volume of the space rises exponentially, thus making the dataset quickly sparse. This phenomenon is also known as the \textit{Curse of Dimensionality} \cite{exp:curse-of-dim}.

Due to the expected size of dataset we will be using, and usage of Feature DTW transformation, we expect quite high-dimensional data. Therefore we are not considering the most common dimensionality techniques such as \textit{Gaussian Mixture Models} \cite{vis:gauss-mixt} and \textit{Mean Shift} \cite{vis:mean-shift}, as they make assumptions about the underlying data distribution, and can not cope with high dimensional data well enough.

\subsubsection{DBSCAN}
The first density-based clustering algorithm in our work is the \textit{Density-based spatial clustering of applications with noise (DBSCAN)} \cite{vis:dbscan}. This algorithm separates the dataset into dense clusters divided by sparse regions and outlying data points, considered as noise. Firstly it separates data points into three groups: core points, non-core points close to the core points, and noise.

We say that a point is a core point if and only if in his surrounding area with radius $\epsilon$ are at least $X$ other data points (the minimal number of data poitns). The non-core points close to the core points have a core point within the radius $\epsilon$, yet they do not meet the minimal data points requirement within $\epsilon$. Noise points do not have any core points in their surrounding area. Hence, the DBSCAN algorithm automatically detects the number of clusters and only requires the radius $\epsilon$ and the minimal number of samples $X$. The disadvantage of DBSCAN is that it can not detect clusters with a very different density as the radius is same for every point.

\subsubsection{OPTICS}
\textit{The Ordering points to identify the clustering structure (OPTICS)} \cite{vis:optics} is a partial solution for DBSCAN's problem with clustering regions with different densities. OPTICS uses the minimal number of neighbors $X$, and instead of a single radius like in DBSCAN, it uses the maximal radius to consider $\epsilon$. For each point it computes the core distance, the minimal radius in which is the point considered as a core point, and reachability distance to every other point, which is either the core distance of the other point or the distance between them, whichever is bigger. If two neighbor points have a reachability distance smaller than the core distance, we consider them to be a part of the same cluster.

Because OPTICS computes both core distance and reachability, it has a much higher computational cost than DBSCAN. With the usage of spatial indexing trees, we could avoid a costly computation of the full distance matrix, making it applicable to larger datasets.

\subsubsection{HDBSCAN}
The \textit{Hierarchical Density-based Spatial Clustering of Applications with Noise (HDBSCAN)} \cite{vis:hdbscan} is another clustering algorithm originating in the DBSCAN. Like DBSCAN and OPTICS, it starts by computing the core distance of every point, which is a minimum radius in which there are at least $X$ data points (minimal number of data points) and the mutual reachability distance for every pair of data points -- the largest value among their core distances and their mutual distance.
\begin{figure}[ht]
     \centering
     \begin{subfigure}[b]{0.495\textwidth}
        \centering
        \includesvg[width=\textwidth]{img/hdbscan_dataset.svg}
        \caption{}
        \label{fig:hdbscan_dataset}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.495\textwidth}
        \centering
        \includesvg[width=\textwidth]{img/hdbscan_tree.svg}
        \caption{}
        \label{fig:hdbscan_tree}
     \end{subfigure}
    \caption{The minimal spanning tree from HDBSCAN (\ref{fig:hdbscan_tree}) using the mutual reachability distance on an artificially generated dataset (\ref{fig:hdbscan_dataset}).
    }
\end{figure}

Afterward, it finds the minimal spanning tree of a weighted graph where data points are vertices, and edges between them have the weight of their mutual reachability distance (Fig.~\ref{fig:hdbscan_tree}).

The next step clusters the tree vertices by single-linkage hierarchical clustering using the edge weights. In Fig.~\ref{fig:hdbscan_hier}, we can see the dendrogram visualizing such clustering where on the $y$ axis we can see the \textit{mutual reachability distance} at which the nodes merge.
The next step converts the tree into a hierarchy of connected components by sorting the edges and merging them in ascending order (Fig.~\ref{fig:hdbscan_hier}). 
\begin{figure}[ht]
     \centering
     \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includesvg[width=\textwidth]{img/hdbscan_hier.svg}
        \caption{}
        \label{fig:hdbscan_hier}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includesvg[width=\textwidth]{img/hdbscan_cons.svg}
        \caption{}
        \label{fig:hdbscan_cons}
     \end{subfigure}
    \caption{\textit{HDBSCAN}'s single linkage hierarchy of connected components (\ref{fig:hdbscan_hier}) and the most stable clusters in the tree and compressed (\ref{fig:hdbscan_cons}).
    }
\end{figure}
To determine the number of clusters in the data, DBSCAN computes the stability of every cluster. As a measure, we are using the $\lambda$, which is reversed \textit{mutual reachability distance} ($\lambda = \frac{1}{distance}$). For every cluster, HDBSCAN defines the distance when it is created $\lambda_c$ and the distance when it splits into multiple clusters $\lambda_d$. Then for every point $p$ in the cluster, it computes the value $\lambda_p$, which is the value when the point leaves the cluster either by cluster split ($\lambda_d$) or complete separation. The stability of a cluster $C$ is then defined as:
\begin{equation}
    \sum_{p \in C}(\lambda_p - \lambda_c)
\end{equation}
Finally, it travels through the tree, starting from the leaves going up to the root node. For every node (cluster), if its stability is greater than the sum of its children's stabilities, HDBSCAN marks the cluster as a real cluster and demarks all of its descendants. Once the algorithm reaches the root node, it stops and returns all marked clusters as final data clustering (Fig.~\ref{fig:hdbscan_cons}).

Even though HDBSCAN consists of multiple computational steps, there are highly optimized implementations \cite{vis:hdbscan-imp} allowing HDBSCAN to be effectively used on large datasets.

\section{Anomaly Detection}
In the last part of our analysis, we want to detect and study the anomalous data points within a dataset. We will focus on two methods \textit{Isolation Forest} and \textit{Lightweight On-line Detector of Anomalies} which are widely used and efficient on large high-dimensional datasets.

\subsection{Isolation Forest}
One of the widely used anomaly detection algorithms for large datasets is the \textit{Isolation Forest} \cite{vis:isoforest}. The central concept of Isolation Forest is that anomalous data points can be isolated faster than the rest of the dataset. 

To find the anomalies, it constructs the forest of binary isolation trees. A binary isolation tree is a simple decision tree with restricted depth where every node of the tree selects the random feature and random value in its range as a separator. The anomaly score of a data point is the average depth it reached in trees in the isolation forest.

Because of its simple nature, the Isolation Forest scales very well with the number of samples and number of features while also achieving competitive results.

\subsection{Lightweight On-line Detector of Anomalies}
The \textit{Lightweight On-line Detector of Anomalies (LODA)} is an anomaly detection method designed to process huge high-dimension real-time data streams. LODA uses a combination of randomly generated sparse projections and histograms to determine the anomaly score of every data point.

In the first step, LODA generates $X$ random projections vectors with only $\sqrt{d}$ non-zero taken from $N(0, 1)$ values where $d$ is input data's dimensionality. Then, it constructs a histogram on every projection vector. The final anomaly score of every point is the negative average logarithm of probability throughout all histograms.

Because LODA is using sparse projections and histograms as very simple density estimators, it has very fast training and execution time. Another advantage is that thru the vectors' sparsity, it is possible to discover which feature contributed most to the data points anomaly score. We compute the \textit{one-tailed two-sample t-test} between probabilities from histograms on projections with and without a specific feature for every feature. The higher the value of the \textit{t-test}, the stronger is the feature significance.


\section{Chapter Summary}
In this chapter, we discussed distance measures for time series. Among them, shaped-based distances, particularly the Euclidian distance and Dynamic Time Warping (DTW) seem like the most promising ones. The main drawback of DTW is its semi-metric nature. To use the advantages of DTW in a metric space, we can use Feature DTW transformation to create a feature space representation of DTW. This representation then forms the input for other machine learning algorithms and can be combined with other distance measures and feature extraction algorithms.

There are several options for clustering of large datasets:
\begin{itemize}
    \item \textit{K-means} if we know the number of clusters and want evenly sized convex clusters. It is a good starting technique as it scales very well.
    \item \textit{Hierarchical agglomerative clustering} is suitable on medium to large datasets based on the chosen linkage. \textit{Ward linkage} produces the most evenly sized clusters but has the highest complexity and is defined only in the euclidian space. For non-euclidian cases, the \textit{average linkage} is a good alternative. While the \textit{single linkage} is very efficient and can be used on large datasets, with the capability to produce non-convex clusters, it is fragile to the noise in the data.
    \item \textit{OPTICS} and \textit{HDBSCAN} for cases when we do not know the number of clusters in our data but want to specify only the minimal number of clusters. As these methods are density-based, we have to be aware of the \textit{curse of dimensionality} consider transformation the dataset into a lower-dimensional representation.
\end{itemize}

In the field of anomaly detection, we recommend the \textit{Isolation Forest} and \textit{LODA}. Both methods are ensembles of simple estimators, giving them high-speed performance regardless of the dataset's size. \textit{LODA} also provides the capability for scoring the features based on their significance in anomaly score.
